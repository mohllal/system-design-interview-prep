# Latency and Throughput

Latency and throughput are two metrics that measure the performance of a computer network. They both work together to deliver high network connectivity and performance. As both impact the transmission of data packets, they also affect one another.

## Latency

Latency is the delay in network communication. It shows the time that data takes to transfer across the network.
Networks with a longer delay or lag have high latency, while those with fast response times have lower latency.

Latency is mesured in milliseconds. The lower the number in milliseconds, the faster the network is performing and vise versa.

## Throughput

Throughput refers to the average volume of data that can actually pass through the network over a specific time. It indicates the number of data packets that arrive at their destinations successfully and the data packet loss.

Throuhput is measured in bits per second (bps) or even megabytes per second (MBps).

Bandwidth represents the total volume of data that can be transfer over a network. So bandwidth can be considered as the theoretical maximum throughput of a network.

## Improving latency and throughput

Shortening the propagation, distance, between the source and destination can improve latency while increasing the overall network bandwidth can improve throughput.

Some other things that can improve latency and throughput

- Caching: storing frequently accessed data geographically closer to the user.

- Choosing the right transport protocols: TCP has higher latency and higher throughput while UDP has lower latency but a higher throughput.

## External references

- [Whatâ€™s the Difference Between Throughput and Latency?](https://aws.amazon.com/compare/the-difference-between-throughput-and-latency/)
